{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#54B1FF\">Preprocessing:</span> &nbsp; <span style=\"color:#1B3EA9\"><b>Scaling</b></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Scaling** refers to the transformation of a range of data. For example, if the data originally range from 100 to 1000, one could scale them to a range of 0 to 1. This type of scaling is important for some machine learning (ML) algorithms, which expect data to lie within a certain range of values.\n",
    "\n",
    "As a practical example to demonstrate the importance of scaling, consider a self-driving car that uses streaming video as an input feature to a machine learning self-driving algorithm.  Imagine that pixel brightness values in the **training set** range from 100 (road) to 255 (bright sun). If the machine learning goal is to follow white markings on the road, the algorithm will likely need to find pixels with a brightness value of about 125. If these data were scaled to the range 0 (minimum brightness) to +1 (maximum brightness) the white-line target brightness value would be about 0.16.\n",
    "\n",
    "Now imagine that car is on the road in real-life, and using a real-time video stream as the **test set**. Imagine that the camera malfunctions, and that a few video pixels return values of 10,000. If you scale these new data to a range of 0 to +1, the new white-line target brightness value would be about 0.003. This would almost certainly disrput the self-driving algorithm, and possibly result in a crash or worse.\n",
    "\n",
    "This example, while simple, demonstrates the importance of being able to **apply a specific processing procedure to the test set**, regardless of the nature of the test set.\n",
    "\n",
    "We shall first consider simple data scaling, then we will consider how to apply a specific scaling to the test set.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Scaling basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 104.88135039  571.51893664 1060.27633761]\n",
      " [ 104.4883183   542.36547993 1064.58941131]\n",
      " [  93.75872113  589.17730008 1096.36627605]\n",
      " [  88.34415188  579.17250381 1052.88949198]\n",
      " [ 106.80445611  592.55966383 1007.10360582]\n",
      " [  58.71292997  502.02183974 1083.26198455]\n",
      " [ 127.81567509  587.00121482 1097.86183422]\n",
      " [ 129.91585642  546.14793623 1078.05291763]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x    = [50, 500, 1000] + 100 * np.random.rand(8, 3)\n",
    "\n",
    "print( x )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1431378   0.2648207  -0.2647958 ]\n",
      " [ 0.12463907 -0.72837546 -0.10778448]\n",
      " [-0.38036788  0.86640351  1.04900711]\n",
      " [-0.6352139   0.52556142 -0.53370342]\n",
      " [ 0.23365209  0.98163344 -2.20047352]\n",
      " [-2.02985836 -2.10279732  0.571964  ]\n",
      " [ 1.22258127  0.79226893  1.10345078]\n",
      " [ 1.32142992 -0.59951523  0.38233533]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_scaled = preprocessing.scale(x)\n",
    "\n",
    "print( x_scaled )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "After scaling, the mean and variance of each feature are 0 and 1, respectively, just like the Standard Normal distribution:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0. -0. -0.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( np.around( x_scaled.mean(axis=0) , 5 ) )\n",
    "\n",
    "print( x_scaled.std(axis=0) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "However, this does NOT mean that the data are now normally distributed. Note that the code above uses `np.random.rand` (i.e., the uniform distribution) and not `np.random.randn` (i.e., the standard normal distribution).\n",
    "\n",
    "This can be seen more clearly for a dataset with many more observations, let's try 1000 observations:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0. -0.  0.]\n",
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x    = [50, 500, 1000] + 100 * np.random.rand(1000, 3)\n",
    "x_scaled = preprocessing.scale(x)\n",
    "\n",
    "print( np.around( x_scaled.mean(axis=0) , 5 ) )\n",
    "print( x_scaled.std(axis=0) )\n",
    "\n",
    "plt.figure()\n",
    "plt.hist( x_scaled[:,0] )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Even though the data now have a mean of zero and a standard deviation of 1, they are clearly **not** normally distributed.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Scaling training and test sets \n",
    "\n",
    "As mentioned at the top of this notebook, it can occasionally be important to scale the **test set** data in exactly the same manner in which the **training set** was scaled.\n",
    "\n",
    "**sklearn** provides a variety of tools to ensure that preprocessing routines are applied equivalently to the training and test sets. Below is an example.\n",
    "\n",
    "<br>\n",
    "\n",
    "First let's define one training set and one test set:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97861834 0.79915856]\n",
      " [0.46147936 0.78052918]\n",
      " [0.11827443 0.63992102]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train  = np.random.rand(10, 2)\n",
    "x_test   = np.random.rand(3, 2)\n",
    "\n",
    "print( x_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "One could separately scaled the training and test sets like this:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.29846443  0.83408861]\n",
      " [-0.16395642  0.57200683]\n",
      " [-1.13450801 -1.40609544]]\n"
     ]
    }
   ],
   "source": [
    "x_train_scaled  = preprocessing.scale(x_train)\n",
    "x_test_scaled   = preprocessing.scale(x_test)\n",
    "\n",
    "print( x_test_scaled )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Or one could scale the training set, then use the results to scale the test set, like this:\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.60475663  0.62226019]\n",
      " [-0.20700717  0.54824228]\n",
      " [-1.40940402 -0.0104192 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler        = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_test_scaled = scaler.transform( x_test )\n",
    "\n",
    "print( x_test_scaled )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "These results are not very different from using `preprocessing.scale`.\n",
    "\n",
    "(Note that `scaler` means \"an object that scales data\", and not \"scalar\".)\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "If the `preprocessing.scale` and `preprocessing.StandardScaler` approaches have produced similar results, why does this matter?\n",
    "\n",
    "It matters because the test_set might not always behave as expected. For example, similar to the self-driving car example above, imagine that one of the **test set** features is strange, with values much higher than the expected maximum of 1:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97861834 10.79915856]\n",
      " [ 0.46147936 10.78052918]\n",
      " [ 0.11827443 10.63992102]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test[:,1] += 10\n",
    "\n",
    "print( x_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now there will be a much bigger difference between the two methods of scaling:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.29846443  0.83408861]\n",
      " [-0.16395642  0.57200683]\n",
      " [-1.13450801 -1.40609544]]\n",
      "\n",
      "[[ 1.60475663 40.35405773]\n",
      " [-0.20700717 40.28003983]\n",
      " [-1.40940402 39.72137835]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test_scaled_0   = preprocessing.scale(x_test)\n",
    "x_test_scaled_1   = scaler.transform( x_test )\n",
    "\n",
    "print( x_test_scaled_0 )\n",
    "print()\n",
    "print( x_test_scaled_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Note that `scaler` has kept the good feature (the first column of data) exactly like before, despite the corrputed second feature.\n",
    "\n",
    "In contrast, `preprocessing.scale` has distorted both features.\n",
    "\n",
    "While this example is simple, it illustrates an important point:\n",
    "\n",
    "When applying various data preprocessing routines, you must understand and justify how those preprocessing routines should be applied to the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
