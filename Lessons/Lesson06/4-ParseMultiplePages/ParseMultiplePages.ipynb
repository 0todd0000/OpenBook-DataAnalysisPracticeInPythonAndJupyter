{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#54B1FF\">Exploring Data:</span> &nbsp; <span style=\"color:#1B3EA9\"><b>Parsing Multiple Pages</b></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook we saw how to parse a single HTML page. Let's replicate that code here:\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 237, 205, 187, 110, 245, 224, 264, 104, 408, 220, 53, 162, 340, 262, -1, 706, 215, 110, 107, 648, 248, 240, 378, 386, 39, 358, 216, 225, 334, 244, 330, 156, 320, 110, 252, 496, 128, 286, 372]\n",
      "\n",
      "[1075, 1790, 2379, 2364, 960, 1219, 8491, 3205, 4729, 3033, 2352, 1581, 1801, 5374, 4325, 4526, 7192, 14542, 960, 834, 5388, 2385, 5136, 5408, 5374, 1076, 11060, 5900, 2567, 5152, 5374, 5374, 2153, 4531, 836, 4182, 1089, 1766, 5981, 4194]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lxml.html\n",
    "\n",
    "def parse_name_node(node):\n",
    "    return node.text\n",
    "\n",
    "def parse_price_node(node):\n",
    "    s = node.text\n",
    "    x = s[1:].replace(',', '')\n",
    "    return int( x )\n",
    "\n",
    "def parse_page_count_node(node):\n",
    "    s = node.text\n",
    "    if 'ページ' in s:\n",
    "        x = s.split(':')[1].split(',')[0]\n",
    "    else:\n",
    "        x = -1  # assign a value of -1 if \"ページ\" is not in the text field\n",
    "    return int( x )\n",
    "\n",
    "\n",
    "# specify data directory:\n",
    "dir0        = os.path.abspath('')              # directory in which this notebook is saved\n",
    "dirLesson   = os.path.dirname( dir0 )          # Lesson directory\n",
    "dirData     = os.path.join(dirLesson, 'Data')  # Data directory\n",
    "\n",
    "# parse the HTML file for name, page and price nodes:\n",
    "fnameHTML   = os.path.join(dirData, 'page1.html')\n",
    "tree        = lxml.html.parse(fnameHTML)\n",
    "body        = tree.find('body') \n",
    "box         = body.find_class('itemCatBox')[0]\n",
    "name_nodes  = box.find_class('name')\n",
    "page_nodes  = box.find_class('itemCatsetsumei')\n",
    "price_nodes = box.find_class('itemCatPrice')\n",
    "\n",
    "# parse the entries:\n",
    "title       = [parse_name_node( node )  for node in name_nodes]\n",
    "pages       = [parse_page_count_node( node )  for node in page_nodes]\n",
    "price       = [parse_price_node( node)  for node in price_nodes]\n",
    "\n",
    "print(pages)\n",
    "print()\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To parse multiple pages, it is most convenient to create a new function, that can be used to parse an entire page. Let's create a `parse_page` function that will do just that:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 237, 205, 187, 110, 245, 224, 264, 104, 408, 220, 53, 162, 340, 262, -1, 706, 215, 110, 107, 648, 248, 240, 378, 386, 39, 358, 216, 225, 334, 244, 330, 156, 320, 110, 252, 496, 128, 286, 372]\n",
      "\n",
      "[1075, 1790, 2379, 2364, 960, 1219, 8491, 3205, 4729, 3033, 2352, 1581, 1801, 5374, 4325, 4526, 7192, 14542, 960, 834, 5388, 2385, 5136, 5408, 5374, 1076, 11060, 5900, 2567, 5152, 5374, 5374, 2153, 4531, 836, 4182, 1089, 1766, 5981, 4194]\n"
     ]
    }
   ],
   "source": [
    "def parse_page(fnameHTML):\n",
    "    tree        = lxml.html.parse(fnameHTML)\n",
    "    body        = tree.find('body') \n",
    "    box         = body.find_class('itemCatBox')[0]\n",
    "    name_nodes  = box.find_class('name')\n",
    "    page_nodes  = box.find_class('itemCatsetsumei')\n",
    "    price_nodes = box.find_class('itemCatPrice')\n",
    "\n",
    "    # parse the entries:\n",
    "    title       = [parse_name_node( node )  for node in name_nodes]\n",
    "    pages       = [parse_page_count_node( node )  for node in page_nodes]\n",
    "    price       = [parse_price_node( node)  for node in price_nodes]\n",
    "\n",
    "    return title, pages, price\n",
    "\n",
    "title,pages,price = parse_page(fnameHTML)\n",
    "\n",
    "print(pages)\n",
    "print()\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Great!  We're now ready to parse multiple pages. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "\n",
      "[1075, 1790, 2379, 2364, 960, 1219, 8491, 3205, 4729, 3033, 2352, 1581, 1801, 5374, 4325, 4526, 7192, 14542, 960, 834, 5388, 2385, 5136, 5408, 5374, 1076, 11060, 5900, 2567, 5152, 5374, 5374, 2153, 4531, 836, 4182, 1089, 1766, 5981, 4194, 3447, 734, 8621, 8976, 1890, 4600, 1234, 5053, 2363, 1320, 1759, 2358, 734, 1770, 1650, 6336, 5393, 1906, 5971, 836, 2066, 2402, 4307, 2399, 2094, 3582, 2329, 14055, 2389, 1905, 5971, 5169, 3607, 5374, 2364, 4179, 5413, 769, 4927, 11281, 1804, 1539, 2085, 3751, 3607, 2987, 1067, 2984, 1190, 474, 1949, 11770, 5971, 834, 18498, 2045, 3251, 815, 2261, 2400, 1551, 2554, 1965, 838, 2388, 5044, 837, 4775, 1018, 12320, 4190, 1164, 2280, 469, 3706, 5354, 269, 5374, 9809, 1803, 3556, 5374, 816, 5080, 2069, 1219, 3322, 1430, 2461, 1099, 7456, 2384, 2200, 4583, 2563, 7457, 8360, 5971, 2045, 3425, 4012, 2277, 2286, 1189, 6368, 2264, 2079, 5019, 4776, 4776, 1936, 1441, 2767, 2143, 3978, 7218, 4576, 5969, 4777, 1099, 3072, 414, 2072, 2207, 2340, 5772, 4470, 2400, 3045, 4902, 2373, 835, 5374, 5971, 5395, 1544, 776, 1422, 6747, 1800, 3584, 4550, 7642, 5374, 1552, 5794, 2389, 5176, 4187, 3589, 3715, 4941, 1188, 2092, 4201, 989, 838, 3698, 956, 14047]\n"
     ]
    }
   ],
   "source": [
    "title  = []\n",
    "pages  = []\n",
    "price  = []\n",
    "\n",
    "for i in range(5):\n",
    "    fnameHTML = os.path.join(dirData, f'page{i+1}.html')\n",
    "    s,n,x     = parse_page(fnameHTML)\n",
    "    title    += s\n",
    "    pages    += n\n",
    "    price    += x\n",
    "\n",
    "\n",
    "print( len(title) )\n",
    "print( len(pages) )\n",
    "print( len(price) )\n",
    "print()\n",
    "print(price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Excellent!  We have parsed all five pages, and have successfully stored the title, page count, and price for all 200 books!\n",
    "\n",
    "⚠️ Often you will encounter formatting problems when dealing with multiple files!  In this case you will need to adjust your parsing functions to deal with different problems. In this case, we were luck that all entries on all pages could be handled successfully by the parsers we developed for **page1.html**\n",
    "\n",
    "<br>\n",
    "\n",
    "In the next notebook, we'll write the parsed results to file.\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
